{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Benis3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJqDNlIPjdkl"
      },
      "source": [
        "from lxml import etree\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "def load_sentirueval_2016(file_name: str) -> Tuple[List[str], List[str]]:\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_name, mode='rb') as fp:\n",
        "        xml_data = fp.read()\n",
        "    root = etree.fromstring(xml_data)\n",
        "    for database in root.getchildren():\n",
        "        if database.tag == 'database':\n",
        "            for table in database.getchildren():\n",
        "                if table.tag != 'table':\n",
        "                    continue\n",
        "                new_text = None\n",
        "                new_label = None\n",
        "                for column in table.getchildren():\n",
        "                    if column.get('name') == 'text':\n",
        "                        new_text = str(column.text).strip()\n",
        "                        if new_label is not None:\n",
        "                            break\n",
        "                    elif column.get('name') not in {'id', 'twitid', 'date'}:\n",
        "                        if new_label is None:\n",
        "                            label_candidate = str(column.text).strip()\n",
        "                            if label_candidate == '-1':\n",
        "                                new_label = 0\n",
        "                            elif label_candidate == '0':\n",
        "                                new_label = 1\n",
        "                            elif label_candidate == '1':\n",
        "                                new_label = 2\n",
        "                                if new_text is not None:\n",
        "                                    break\n",
        "                if (new_text is None) or (new_label is None):\n",
        "                    raise ValueError('File `{0}` contains some error!'.format(file_name))\n",
        "                texts.append(new_text)\n",
        "                labels.append(new_label)\n",
        "            break\n",
        "    return texts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iREqqkf6iFAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6973f70a-25c9-496b-baec-d7326a3ad22a"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y9GJyKxq82G"
      },
      "source": [
        "texts, labels = load_sentirueval_2016('drive/MyDrive/bank_train_2016.xml')\n",
        "texts1, labels1 = load_sentirueval_2016('drive/MyDrive/tkk_train_2016.xml')\n",
        "texts.extend(texts1)\n",
        "labels.extend(labels1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_for_testing, labels_for_testing = load_sentirueval_2016('drive/MyDrive/banks_test_etalon.xml')\n",
        "texts_for_testing1, labels_for_testing1 = load_sentirueval_2016('drive/MyDrive/tkk_test_etalon.xml')\n",
        "texts_for_testing.extend(texts_for_testing1)\n",
        "labels_for_testing.extend(labels_for_testing1)"
      ],
      "metadata": {
        "id": "l2aukfPogXr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f ft_native_300_ru_twitter_nltk_word_tokenize.bin\n",
        "!wget http://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYQwUPx3PkGU",
        "outputId": "4e963a85-f2fe-4755-86a6-4df216db1cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-03 16:25:48--  http://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.bin\n",
            "Resolving files.deeppavlov.ai (files.deeppavlov.ai)... 178.63.27.41\n",
            "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|178.63.27.41|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.bin [following]\n",
            "--2022-01-03 16:25:48--  https://files.deeppavlov.ai/embeddings/ft_native_300_ru_twitter_nltk_word_tokenize.bin\n",
            "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|178.63.27.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3417475450 (3.2G) [application/octet-stream]\n",
            "Saving to: ‘ft_native_300_ru_twitter_nltk_word_tokenize.bin’\n",
            "\n",
            "ft_native_300_ru_tw 100%[===================>]   3.18G  21.6MB/s    in 2m 15s  \n",
            "\n",
            "2022-01-03 16:28:04 (24.1 MB/s) - ‘ft_native_300_ru_twitter_nltk_word_tokenize.bin’ saved [3417475450/3417475450]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzfIDxS_s8Xo"
      },
      "source": [
        "import gensim\n",
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "\n",
        "fasttext_model = FastText()\n",
        "fasttext_model.file_name = 'ft_native_300_ru_twitter_nltk_word_tokenize.bin'\n",
        "fasttext_model.load_binary_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "x_train = []\n",
        "for text in texts:\n",
        "  x_train.append(fasttext_model.wv[text])\n",
        "x_train = np.array(x_train, dtype=np.float32)\n",
        "x_train = x_train.reshape(len(texts), 100, 1, 1)\n",
        "\n",
        "x_test = []\n",
        "for text in texts_for_testing:\n",
        "  x_test.append(fasttext_model.wv[text])\n",
        "x_test = np.array(x_test, dtype=np.float32)\n",
        "x_test = x_test.reshape(len(texts_for_testing), 100, 1, 1)\n",
        "\n",
        "y_train = np.array(labels)\n",
        "y_test = np.array(labels_for_testing)"
      ],
      "metadata": {
        "id": "98IgMMIV9dDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAfQRg48ahil"
      },
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yEUyUi7ahi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1f81b6-dc6a-4585-9102-079d67ad150f"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Reshape\n",
        "from tensorflow.keras.initializers import he_uniform, glorot_uniform\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D, SpatialDropout2D\n",
        "\n",
        "\n",
        "cnn = Sequential()\n",
        "cnn.add(Conv2D(32, (3, 1), padding='same', activation='relu', input_shape=x_train.shape[1:],\n",
        "               kernel_initializer=he_uniform(seed=RANDOM_SEED), name='Conv_Block1_Layer1'))\n",
        "cnn.add(Conv2D(32, (3, 1), activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),\n",
        "               name='Conv_Block1_Layer2'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 1), name='MaxPool1'))\n",
        "cnn.add(SpatialDropout2D(rate=0.15, name='SpatialDropout1', seed=RANDOM_SEED))\n",
        "\n",
        "cnn.add(Conv2D(64, (3, 1), padding='same', activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),\n",
        "              name='Conv_Block2_Layer1'))\n",
        "cnn.add(Conv2D(64, (3, 1), activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),\n",
        "               name='Conv_Block2_Layer2'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 1), name='MaxPool2'))\n",
        "cnn.add(SpatialDropout2D(rate=0.15, name='SpatialDropout2', seed=RANDOM_SEED))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(512, activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED), name='HiddenLayer'))\n",
        "cnn.add(Dropout(rate=0.5, seed=RANDOM_SEED, name='DropoutAfterHidden'))\n",
        "cnn.add(Dense(10, activation='softmax', kernel_initializer=glorot_uniform(seed=RANDOM_SEED), name='OutputLayer'))\n",
        "cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
        "cnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Conv_Block1_Layer1 (Conv2D)  (None, 100, 1, 32)       128       \n",
            "                                                                 \n",
            " Conv_Block1_Layer2 (Conv2D)  (None, 98, 1, 32)        3104      \n",
            "                                                                 \n",
            " MaxPool1 (MaxPooling2D)     (None, 49, 1, 32)         0         \n",
            "                                                                 \n",
            " SpatialDropout1 (SpatialDro  (None, 49, 1, 32)        0         \n",
            " pout2D)                                                         \n",
            "                                                                 \n",
            " Conv_Block2_Layer1 (Conv2D)  (None, 49, 1, 64)        6208      \n",
            "                                                                 \n",
            " Conv_Block2_Layer2 (Conv2D)  (None, 47, 1, 64)        12352     \n",
            "                                                                 \n",
            " MaxPool2 (MaxPooling2D)     (None, 23, 1, 64)         0         \n",
            "                                                                 \n",
            " SpatialDropout2 (SpatialDro  (None, 23, 1, 64)        0         \n",
            " pout2D)                                                         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1472)              0         \n",
            "                                                                 \n",
            " HiddenLayer (Dense)         (None, 512)               754176    \n",
            "                                                                 \n",
            " DropoutAfterHidden (Dropout  (None, 512)              0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " OutputLayer (Dense)         (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 781,098\n",
            "Trainable params: 781,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCfOIa6dahi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3f6fa1-3066-4a0f-c487-9d1929d4f05e"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "cnn.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x_val, y_val),\n",
        "    shuffle=True, epochs=100,\n",
        "    callbacks=[\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss', patience=5, restore_best_weights=True, verbose=1\n",
        "        )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "143/143 [==============================] - 3s 12ms/step - loss: 0.8339 - sparse_categorical_accuracy: 0.6233 - val_loss: 0.7347 - val_sparse_categorical_accuracy: 0.6516\n",
            "Epoch 2/100\n",
            "143/143 [==============================] - 1s 10ms/step - loss: 0.7619 - sparse_categorical_accuracy: 0.6520 - val_loss: 0.7150 - val_sparse_categorical_accuracy: 0.6787\n",
            "Epoch 3/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.7435 - sparse_categorical_accuracy: 0.6654 - val_loss: 0.7066 - val_sparse_categorical_accuracy: 0.6821\n",
            "Epoch 4/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.7327 - sparse_categorical_accuracy: 0.6724 - val_loss: 0.7025 - val_sparse_categorical_accuracy: 0.6841\n",
            "Epoch 5/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.7231 - sparse_categorical_accuracy: 0.6758 - val_loss: 0.6948 - val_sparse_categorical_accuracy: 0.6920\n",
            "Epoch 6/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.7149 - sparse_categorical_accuracy: 0.6797 - val_loss: 0.6980 - val_sparse_categorical_accuracy: 0.6865\n",
            "Epoch 7/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.7061 - sparse_categorical_accuracy: 0.6820 - val_loss: 0.6928 - val_sparse_categorical_accuracy: 0.6939\n",
            "Epoch 8/100\n",
            "143/143 [==============================] - 2s 12ms/step - loss: 0.6927 - sparse_categorical_accuracy: 0.6901 - val_loss: 0.6892 - val_sparse_categorical_accuracy: 0.6939\n",
            "Epoch 9/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6923 - sparse_categorical_accuracy: 0.6914 - val_loss: 0.6946 - val_sparse_categorical_accuracy: 0.6964\n",
            "Epoch 10/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6864 - sparse_categorical_accuracy: 0.6983 - val_loss: 0.6856 - val_sparse_categorical_accuracy: 0.7018\n",
            "Epoch 11/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6778 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.6820 - val_sparse_categorical_accuracy: 0.7008\n",
            "Epoch 12/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6729 - sparse_categorical_accuracy: 0.6995 - val_loss: 0.6824 - val_sparse_categorical_accuracy: 0.6944\n",
            "Epoch 13/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6636 - sparse_categorical_accuracy: 0.7089 - val_loss: 0.6773 - val_sparse_categorical_accuracy: 0.7023\n",
            "Epoch 14/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6620 - sparse_categorical_accuracy: 0.7122 - val_loss: 0.6710 - val_sparse_categorical_accuracy: 0.6984\n",
            "Epoch 15/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6497 - sparse_categorical_accuracy: 0.7150 - val_loss: 0.6815 - val_sparse_categorical_accuracy: 0.6989\n",
            "Epoch 16/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6443 - sparse_categorical_accuracy: 0.7183 - val_loss: 0.6800 - val_sparse_categorical_accuracy: 0.6969\n",
            "Epoch 17/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6392 - sparse_categorical_accuracy: 0.7212 - val_loss: 0.6846 - val_sparse_categorical_accuracy: 0.7068\n",
            "Epoch 18/100\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6327 - sparse_categorical_accuracy: 0.7249 - val_loss: 0.6823 - val_sparse_categorical_accuracy: 0.7028\n",
            "Epoch 19/100\n",
            "139/143 [============================>.] - ETA: 0s - loss: 0.6205 - sparse_categorical_accuracy: 0.7299Restoring model weights from the end of the best epoch: 14.\n",
            "143/143 [==============================] - 2s 11ms/step - loss: 0.6203 - sparse_categorical_accuracy: 0.7293 - val_loss: 0.6842 - val_sparse_categorical_accuracy: 0.7033\n",
            "Epoch 00019: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fac4c9081d0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggnajHWOahi2"
      },
      "source": [
        "y_pred = np.argmax(cnn.predict(x_test, batch_size=128), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = ['negative', 'neutral', 'positive']"
      ],
      "metadata": {
        "id": "DzT2n-bX3gOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_WSa_dMahi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b3861f-03fc-42de-dfab-f4cb15e9b908"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=label_names, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.3425    0.4237    0.3788       767\n",
            "     neutral     0.7214    0.7592    0.7398      2238\n",
            "    positive     0.4444    0.0130    0.0252       308\n",
            "\n",
            "    accuracy                         0.6121      3313\n",
            "   macro avg     0.5028    0.3986    0.3813      3313\n",
            "weighted avg     0.6080    0.6121    0.5898      3313\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textattack[tensorflow,optional]"
      ],
      "metadata": {
        "id": "ntzUlSFSLVM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a022be77-5a18-42e9-f626-b85b6daefc4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textattack[optional,tensorflow]\n",
            "  Downloading textattack-0.3.4-py3-none-any.whl (373 kB)\n",
            "\u001b[K     |████████████████████████████████| 373 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (1.4.1)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (1.10.0+cu111)\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (3.2.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (8.12.0)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Collecting flair\n",
            "  Downloading flair-0.10-py3-none-any.whl (322 kB)\n",
            "\u001b[K     |████████████████████████████████| 322 kB 54.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (1.7.1)\n",
            "Collecting lemminflect\n",
            "  Downloading lemminflect-0.2.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting language-tool-python\n",
            "  Downloading language_tool_python-2.6.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (3.4.0)\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Collecting bert-score>=0.3.5\n",
            "  Downloading bert_score-0.3.11-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (0.5.3)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (1.1.5)\n",
            "Collecting lru-dict\n",
            "  Downloading lru-dict-1.1.7.tar.gz (10 kB)\n",
            "Collecting transformers>=3.3.0\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 41.9 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 35.5 MB/s \n",
            "\u001b[?25hCollecting stanza\n",
            "  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n",
            "\u001b[K     |████████████████████████████████| 432 kB 51.2 MB/s \n",
            "\u001b[?25hCollecting sentence-transformers>0.2.6\n",
            "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 2.0 MB/s \n",
            "\u001b[?25hCollecting visdom\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 59.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow-text>=2\n",
            "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (from textattack[optional,tensorflow]) (0.12.0)\n",
            "Collecting tensorflow==2.5.0\n",
            "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.3 MB 15 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==2.5.0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 58.6 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->textattack[optional,tensorflow]) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->textattack[optional,tensorflow]) (5.2.1)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (1.6.3)\n",
            "Collecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (0.37.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (2.7.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (0.4.0)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (3.17.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (3.3.0)\n",
            "Collecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->textattack[optional,tensorflow]) (3.1.0)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack[optional,tensorflow]) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack[optional,tensorflow]) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack[optional,tensorflow]) (21.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0->textattack[optional,tensorflow]) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert-score>=0.3.5->textattack[optional,tensorflow]) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack[optional,tensorflow]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack[optional,tensorflow]) (2018.9)\n",
            "Collecting tokenizers>=0.10.3\n",
            "  Downloading tokenizers-0.11.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>0.2.6->textattack[optional,tensorflow]) (0.11.1+cu111)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>0.2.6->textattack[optional,tensorflow]) (1.0.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 433 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (1.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[optional,tensorflow]) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[optional,tensorflow]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[optional,tensorflow]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score>=0.3.5->textattack[optional,tensorflow]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->textattack[optional,tensorflow]) (3.1.1)\n",
            "Collecting tensorflow-text>=2\n",
            "  Downloading tensorflow_text-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 58.4 MB/s \n",
            "\u001b[?25h  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 39.9 MB/s \n",
            "\u001b[?25h  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack[optional,tensorflow]) (2019.12.20)\n",
            "Collecting tokenizers>=0.10.3\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 64.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 17.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 59.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.16.0-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 51.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 67.7 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.15.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 63.1 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 34.6 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 44.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 51.9 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 60.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.13.0-py3-none-any.whl (285 kB)\n",
            "\u001b[K     |████████████████████████████████| 285 kB 55.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 29.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.12.0-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 32.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 67.9 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[optional,tensorflow]) (0.3.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 57.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.10.1-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 48.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.10.0-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 56.5 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.9.0-py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 47.4 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 54.1 MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.7.0-py3-none-any.whl (234 kB)\n",
            "\u001b[K     |████████████████████████████████| 234 kB 58.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[optional,tensorflow]) (3.0.0)\n",
            "  Downloading datasets-1.6.1-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 58.0 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.6.0-py3-none-any.whl (202 kB)\n",
            "\u001b[K     |████████████████████████████████| 202 kB 58.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 54.2 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 34.8 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.4.0-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 57.7 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 61.3 MB/s \n",
            "\u001b[?25h  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->textattack[optional,tensorflow]) (0.70.12.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair->textattack[optional,tensorflow]) (0.8.9)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 40.0 MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 43.0 MB/s \n",
            "\u001b[?25hCollecting more-itertools\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair->textattack[optional,tensorflow]) (4.2.6)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.4-py3-none-any.whl (19 kB)\n",
            "  Downloading konoha-4.6.3-py3-none-any.whl (19 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[optional,tensorflow]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[optional,tensorflow]) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers>0.2.6->textattack[optional,tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers>0.2.6->textattack[optional,tensorflow]) (3.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair->textattack[optional,tensorflow]) (0.2.5)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->textattack[optional,tensorflow]) (0.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.3.0->textattack[optional,tensorflow]) (7.1.2)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>0.2.6->textattack[optional,tensorflow]) (7.1.2)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom->textattack[optional,tensorflow]) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom->textattack[optional,tensorflow]) (22.3.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->textattack[optional,tensorflow]) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->textattack[optional,tensorflow]) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 51.0 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.1-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 54.7 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: sentence-transformers, wrapt, gdown, mpld3, overrides, sqlitedict, ftfy, langdetect, lru-dict, emoji, visdom, torchfile, subprocess32, pathtools, wikipedia-api, word2number\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=978161b7ff577d34989d92f8a1cbed5a5b6f392724c03ce2c53fc5d31d4bf824\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68718 sha256=1cccfa49b083eac5532758c516f1b3e57efaa4ac7620a1dcf13895cfc68193bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9704 sha256=ba6c534b6d4bccde7198dca1b5739fa55e10d4d8ac775fd2e6b5515662435e5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=f3017ccca4507569994e4dcc248b49c54eda39d6695ea61ae4edbeb46399eb53\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=bf44b2078ef401e522945d07ee1de26a489eeaa9106448957da485707f819884\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=f395e5db07d67b965bcfdc928514b9baf894d8e7a19eed1aa55317d17cfb52c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=e2c8deea2782469cc86b4580db63f0113dda529ca987986f6fb7ac11fa2f3dcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=8c9d1dd80902fd0efda49ab366328e9641a52399bd5194e9b1cfca0e62e32c86\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for lru-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lru-dict: filename=lru_dict-1.1.7-cp37-cp37m-linux_x86_64.whl size=28410 sha256=b6d24f535a48d3968d60c605eb69d74700d1a0ccddfe7b7fe19e15726abf972f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/0b/4e/aa8fec9833090cd52bcd76f92f9d95e1ee7b915c12093663b4\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=fdf6caa576d202c057015236f1dc0fdced3df828d0d582ec3a03c3cf640034f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=f4c1a193abd79ca8fdfba1761a1fd3ecb1dced753f6e183c5c8877b47eb56101\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5710 sha256=467d5dd56d7081ab752b0f179a04fa338f52eecaad3171184f3c6ba6eadf9b2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=ae2d94dcbffc092f66d2a90ef13804b708efeeac96b57f819113359a58304d26\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=60a2c073c4cbe7a871adc4c8ee103563710c3aee9ca4cd155e2cb462256fa838\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13475 sha256=d5a88a4b196b5b62766777a396d32506deac968dff37bb4a6336f32a51b6a392\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5580 sha256=1da0f49033e558178c0ea2bdddfeb0e2fb3645a00c927b20c95bfb6baad0f0f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built sentence-transformers wrapt gdown mpld3 overrides sqlitedict ftfy langdetect lru-dict emoji visdom torchfile subprocess32 pathtools wikipedia-api word2number\n",
            "Installing collected packages: typing-extensions, tqdm, pyyaml, wrapt, tokenizers, smmap, sentencepiece, sacremoses, overrides, huggingface-hub, grpcio, gensim, xxhash, wikipedia-api, transformers, tensorflow-estimator, sqlitedict, segtok, mpld3, more-itertools, langdetect, konoha, keras-nightly, jsonpointer, janome, gitdb, gdown, ftfy, flatbuffers, deprecated, conllu, bpemb, yaspin, word2number, websocket-client, torchfile, terminaltables, tensorflow, subprocess32, shortuuid, sentry-sdk, pathtools, num2words, lru-dict, lemminflect, language-tool-python, jsonpatch, GitPython, flair, emoji, docker-pycreds, datasets, configparser, bert-score, wandb, visdom, textattack, tensorflow-text, tensorboardX, stanza, sentence-transformers\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.3\n",
            "    Uninstalling tqdm-4.62.3:\n",
            "      Successfully uninstalled tqdm-4.62.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.42.0\n",
            "    Uninstalling grpcio-1.42.0:\n",
            "      Successfully uninstalled grpcio-1.42.0\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 bert-score-0.3.11 bpemb-0.3.3 configparser-5.2.0 conllu-4.4.1 datasets-1.2.1 deprecated-1.2.13 docker-pycreds-0.4.0 emoji-1.6.1 flair-0.10 flatbuffers-1.12 ftfy-6.0.3 gdown-3.12.2 gensim-3.8.3 gitdb-4.0.9 grpcio-1.34.1 huggingface-hub-0.2.1 janome-0.4.1 jsonpatch-1.32 jsonpointer-2.2 keras-nightly-2.5.0.dev2021032900 konoha-4.6.3 langdetect-1.0.9 language-tool-python-2.6.2 lemminflect-0.2.2 lru-dict-1.1.7 more-itertools-8.8.0 mpld3-0.3 num2words-0.5.10 overrides-3.1.0 pathtools-0.1.2 pyyaml-6.0 sacremoses-0.0.46 segtok-1.5.11 sentence-transformers-2.1.0 sentencepiece-0.1.95 sentry-sdk-1.5.1 shortuuid-1.0.8 smmap-5.0.0 sqlitedict-1.7.0 stanza-1.3.0 subprocess32-3.5.4 tensorboardX-2.4.1 tensorflow-2.5.0 tensorflow-estimator-2.5.0 tensorflow-text-2.5.0 terminaltables-3.1.10 textattack-0.3.4 tokenizers-0.10.3 torchfile-0.1.0 tqdm-4.49.0 transformers-4.15.0 typing-extensions-3.7.4.3 visdom-0.1.8.9 wandb-0.12.9 websocket-client-1.2.3 wikipedia-api-0.5.4 word2number-1.1 wrapt-1.12.1 xxhash-2.0.2 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
        "from textattack.transformations import WordSwapNeighboringCharacterSwap\n",
        "from textattack.constraints.pre_transformation import RepeatModification\n",
        "from textattack.augmentation import Augmenter\n",
        "\n",
        "\n",
        "def aug_train_data(texts, labels):\n",
        "  transformation = WordSwapNeighboringCharacterSwap()\n",
        "  constraints = [RepeatModification()]\n",
        "  aug = Augmenter(transformation=transformation, pct_words_to_swap=0.5, transformations_per_example=7)\n",
        "  aug_texts = []\n",
        "  aug_labels = []\n",
        "  texts_total = len(texts)\n",
        "  for text, label in zip(texts, labels):\n",
        "    aug_text_options = aug.augment(text)\n",
        "    aug_texts.extend(aug_text_options)\n",
        "    aug_labels.extend([label] * len(aug_text_options))\n",
        "    texts_total = len(texts)\n",
        "  return aug_texts, aug_labels"
      ],
      "metadata": {
        "id": "iYdwbFAw5kkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_texts, aug_labels = aug_train_data(texts, labels)\n",
        "x_train = []\n",
        "for text in aug_texts:\n",
        "  try:\n",
        "    x_train.append(fasttext_model.wv[text])\n",
        "  except KeyError:\n",
        "    x_train.append(np.zeros(100))\n",
        "x_train = np.array(x_train, dtype=np.float32)\n",
        "x_train = x_train.reshape(len(aug_texts), 100, 1, 1)\n",
        "y_train = np.array(aug_labels)"
      ],
      "metadata": {
        "id": "m1nhWYUnsgVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGPAQjszahi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e928bb6a-1cf3-42a3-e1a6-626a198ef412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Conv_Block1_Layer1 (Conv2D)  (None, 100, 1, 32)       128       \n",
            "                                                                 \n",
            " Conv_Block1_Layer2 (Conv2D)  (None, 98, 1, 32)        3104      \n",
            "                                                                 \n",
            " MaxPool1 (MaxPooling2D)     (None, 49, 1, 32)         0         \n",
            "                                                                 \n",
            " SpatialDropout1 (SpatialDro  (None, 49, 1, 32)        0         \n",
            " pout2D)                                                         \n",
            "                                                                 \n",
            " Conv_Block2_Layer1 (Conv2D)  (None, 49, 1, 64)        6208      \n",
            "                                                                 \n",
            " Conv_Block2_Layer2 (Conv2D)  (None, 47, 1, 64)        12352     \n",
            "                                                                 \n",
            " MaxPool2 (MaxPooling2D)     (None, 23, 1, 64)         0         \n",
            "                                                                 \n",
            " SpatialDropout2 (SpatialDro  (None, 23, 1, 64)        0         \n",
            " pout2D)                                                         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1472)              0         \n",
            "                                                                 \n",
            " HiddenLayer (Dense)         (None, 512)               754176    \n",
            "                                                                 \n",
            " DropoutAfterHidden (Dropout  (None, 512)              0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " OutputLayer (Dense)         (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 781,098\n",
            "Trainable params: 781,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cnn_with_augmentation = Sequential()\n",
        "cnn_with_augmentation.add(Conv2D(32, (3, 1), padding='same', activation='relu', input_shape=x_train.shape[1:],\n",
        "                                 kernel_initializer=he_uniform(seed=RANDOM_SEED), name='Conv_Block1_Layer1'))\n",
        "cnn_with_augmentation.add(Conv2D(32, (3, 1), activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),\n",
        "                                 name='Conv_Block1_Layer2'))\n",
        "cnn_with_augmentation.add(MaxPooling2D(pool_size=(2, 1), name='MaxPool1'))\n",
        "cnn_with_augmentation.add(SpatialDropout2D(rate=0.15, name='SpatialDropout1', seed=RANDOM_SEED))\n",
        "cnn_with_augmentation.add(Conv2D(64, (3, 1), padding='same', activation='relu',\n",
        "                                 kernel_initializer=he_uniform(seed=RANDOM_SEED), name='Conv_Block2_Layer1'))\n",
        "cnn_with_augmentation.add(Conv2D(64, (3, 1), activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),\n",
        "                                 name='Conv_Block2_Layer2'))\n",
        "cnn_with_augmentation.add(MaxPooling2D(pool_size=(2, 1), name='MaxPool2'))\n",
        "cnn_with_augmentation.add(SpatialDropout2D(rate=0.15, name='SpatialDropout2', seed=RANDOM_SEED))\n",
        "cnn_with_augmentation.add(Flatten())\n",
        "cnn_with_augmentation.add(Dense(512, activation='relu', kernel_initializer=he_uniform(seed=RANDOM_SEED),\n",
        "                                name='HiddenLayer'))\n",
        "cnn_with_augmentation.add(Dropout(rate=0.5, seed=RANDOM_SEED, name='DropoutAfterHidden'))\n",
        "cnn_with_augmentation.add(Dense(10, activation='softmax', kernel_initializer=glorot_uniform(seed=RANDOM_SEED),\n",
        "                                name='OutputLayer'))\n",
        "cnn_with_augmentation.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
        "cnn_with_augmentation.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "cnn_with_augmentation.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x_val, y_val),\n",
        "    shuffle=True, epochs=100,\n",
        "    callbacks=[\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss', patience=5, restore_best_weights=True, verbose=1\n",
        "        )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "-NLBhijms2Va",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6f6071-2bc9-4c43-98c2-4c335d14bc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "472/472 [==============================] - 9s 15ms/step - loss: 0.7691 - sparse_categorical_accuracy: 0.6621 - val_loss: 0.7362 - val_sparse_categorical_accuracy: 0.6758\n",
            "Epoch 2/100\n",
            "472/472 [==============================] - 6s 12ms/step - loss: 0.7268 - sparse_categorical_accuracy: 0.6775 - val_loss: 0.7057 - val_sparse_categorical_accuracy: 0.6866\n",
            "Epoch 3/100\n",
            "472/472 [==============================] - 5s 10ms/step - loss: 0.7128 - sparse_categorical_accuracy: 0.6848 - val_loss: 0.6965 - val_sparse_categorical_accuracy: 0.6905\n",
            "Epoch 4/100\n",
            "472/472 [==============================] - 5s 10ms/step - loss: 0.7033 - sparse_categorical_accuracy: 0.6892 - val_loss: 0.6886 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 5/100\n",
            "472/472 [==============================] - 4s 10ms/step - loss: 0.6951 - sparse_categorical_accuracy: 0.6934 - val_loss: 0.6894 - val_sparse_categorical_accuracy: 0.6938\n",
            "Epoch 6/100\n",
            "472/472 [==============================] - 5s 10ms/step - loss: 0.6869 - sparse_categorical_accuracy: 0.6970 - val_loss: 0.6770 - val_sparse_categorical_accuracy: 0.6995\n",
            "Epoch 7/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.6807 - sparse_categorical_accuracy: 0.7023 - val_loss: 0.6806 - val_sparse_categorical_accuracy: 0.7011\n",
            "Epoch 8/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.6731 - sparse_categorical_accuracy: 0.7050 - val_loss: 0.6731 - val_sparse_categorical_accuracy: 0.7047\n",
            "Epoch 9/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.6656 - sparse_categorical_accuracy: 0.7091 - val_loss: 0.6692 - val_sparse_categorical_accuracy: 0.7065\n",
            "Epoch 10/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.6582 - sparse_categorical_accuracy: 0.7107 - val_loss: 0.6725 - val_sparse_categorical_accuracy: 0.7112\n",
            "Epoch 11/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.6506 - sparse_categorical_accuracy: 0.7145 - val_loss: 0.6628 - val_sparse_categorical_accuracy: 0.7144\n",
            "Epoch 12/100\n",
            "472/472 [==============================] - 5s 10ms/step - loss: 0.6424 - sparse_categorical_accuracy: 0.7203 - val_loss: 0.6577 - val_sparse_categorical_accuracy: 0.7148\n",
            "Epoch 13/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.6345 - sparse_categorical_accuracy: 0.7246 - val_loss: 0.6651 - val_sparse_categorical_accuracy: 0.7087\n",
            "Epoch 14/100\n",
            "472/472 [==============================] - 4s 10ms/step - loss: 0.6248 - sparse_categorical_accuracy: 0.7271 - val_loss: 0.6654 - val_sparse_categorical_accuracy: 0.7105\n",
            "Epoch 15/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.6176 - sparse_categorical_accuracy: 0.7326 - val_loss: 0.6537 - val_sparse_categorical_accuracy: 0.7218\n",
            "Epoch 16/100\n",
            "472/472 [==============================] - 5s 10ms/step - loss: 0.6080 - sparse_categorical_accuracy: 0.7376 - val_loss: 0.6525 - val_sparse_categorical_accuracy: 0.7242\n",
            "Epoch 17/100\n",
            "472/472 [==============================] - 5s 10ms/step - loss: 0.5979 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.6411 - val_sparse_categorical_accuracy: 0.7248\n",
            "Epoch 18/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.5892 - sparse_categorical_accuracy: 0.7475 - val_loss: 0.6420 - val_sparse_categorical_accuracy: 0.7221\n",
            "Epoch 19/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.5800 - sparse_categorical_accuracy: 0.7522 - val_loss: 0.6395 - val_sparse_categorical_accuracy: 0.7294\n",
            "Epoch 20/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.5694 - sparse_categorical_accuracy: 0.7578 - val_loss: 0.6419 - val_sparse_categorical_accuracy: 0.7272\n",
            "Epoch 21/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.5587 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.6364 - val_sparse_categorical_accuracy: 0.7326\n",
            "Epoch 22/100\n",
            "472/472 [==============================] - 6s 12ms/step - loss: 0.5519 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.6385 - val_sparse_categorical_accuracy: 0.7314\n",
            "Epoch 23/100\n",
            "472/472 [==============================] - 5s 11ms/step - loss: 0.5408 - sparse_categorical_accuracy: 0.7720 - val_loss: 0.6350 - val_sparse_categorical_accuracy: 0.7323\n",
            "Epoch 24/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.5312 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.6343 - val_sparse_categorical_accuracy: 0.7369\n",
            "Epoch 25/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.5200 - sparse_categorical_accuracy: 0.7802 - val_loss: 0.6381 - val_sparse_categorical_accuracy: 0.7254\n",
            "Epoch 26/100\n",
            "472/472 [==============================] - 5s 10ms/step - loss: 0.5146 - sparse_categorical_accuracy: 0.7839 - val_loss: 0.6325 - val_sparse_categorical_accuracy: 0.7385\n",
            "Epoch 27/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.5043 - sparse_categorical_accuracy: 0.7876 - val_loss: 0.6282 - val_sparse_categorical_accuracy: 0.7406\n",
            "Epoch 28/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.4975 - sparse_categorical_accuracy: 0.7895 - val_loss: 0.6190 - val_sparse_categorical_accuracy: 0.7431\n",
            "Epoch 29/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.4895 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.6297 - val_sparse_categorical_accuracy: 0.7418\n",
            "Epoch 30/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.4810 - sparse_categorical_accuracy: 0.7991 - val_loss: 0.6264 - val_sparse_categorical_accuracy: 0.7417\n",
            "Epoch 31/100\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.4731 - sparse_categorical_accuracy: 0.8032 - val_loss: 0.6346 - val_sparse_categorical_accuracy: 0.7433\n",
            "Epoch 32/100\n",
            "472/472 [==============================] - 4s 10ms/step - loss: 0.4621 - sparse_categorical_accuracy: 0.8072 - val_loss: 0.6253 - val_sparse_categorical_accuracy: 0.7385\n",
            "Epoch 33/100\n",
            "472/472 [==============================] - ETA: 0s - loss: 0.4593 - sparse_categorical_accuracy: 0.8095Restoring model weights from the end of the best epoch: 28.\n",
            "472/472 [==============================] - 4s 9ms/step - loss: 0.4593 - sparse_categorical_accuracy: 0.8095 - val_loss: 0.6324 - val_sparse_categorical_accuracy: 0.7417\n",
            "Epoch 00033: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fabd2f59350>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM9Y73aLahi6"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(cnn_with_augmentation.predict(x_test, batch_size=128), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZgPBGrYahi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701abf8e-16b0-4f16-866c-8c8be0fc55dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.4888    0.3954    0.4372     12606\n",
            "     neutral     0.6455    0.7681    0.7015     22747\n",
            "    positive     0.1498    0.0693    0.0948      3563\n",
            "\n",
            "    accuracy                         0.5834     38916\n",
            "   macro avg     0.4280    0.4109    0.4111     38916\n",
            "weighted avg     0.5493    0.5834    0.5603     38916\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=label_names, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Свёрточная нейросеть без аугментации дала более хорошие результаты, чем с аугментацией, и более плохие, чем логистическая регрессия."
      ],
      "metadata": {
        "id": "bMhiQl9HSdNt"
      }
    }
  ]
}